{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a7ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from scipy import signal\n",
    "from scipy.io.wavfile import read, write\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 140\n",
    "from scipy import fft, signal\n",
    "import scipy\n",
    "from scipy.io.wavfile import read\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import fft, fftfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3644d1",
   "metadata": {},
   "source": [
    "# Understanding The Shazam Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1fb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Image(filename=\"shazam_logo.png\", width=300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c3a47",
   "metadata": {},
   "source": [
    "# What does a sound file look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"data/middle_c.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs, song = read(\"data/middle_c.wav\")\n",
    "    \n",
    "time_to_plot = np.arange(0, 1500, dtype=int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.plot(time_to_plot, song[time_to_plot])\n",
    "ax.set_xlabel(\"Time Step\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6164c2",
   "metadata": {},
   "source": [
    "# What does the Fourier Transform of a sound file look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e705bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_song(sampling_rate, song):\n",
    "    num_samples = len(song)\n",
    "    \n",
    "    # apply fourier transform to the song\n",
    "    song_ft = rfft(song)\n",
    "    \n",
    "    # get everything to be real-valued\n",
    "    song_ft = np.abs(song_ft)\n",
    "    \n",
    "    x_axis = rfftfreq(num_samples, 1 / sampling_rate)\n",
    "\n",
    "    return x_axis, song_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs, song = read(\"data/middle_c.wav\")\n",
    "    \n",
    "time_to_plot = np.arange(0, 1500, dtype=int)\n",
    "idx, song_ft = transform_song(Fs, song[time_to_plot])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.set_xlim([0, 2000])\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "plt.plot(idx, song_ft)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07fb3e",
   "metadata": {},
   "source": [
    "### Frequency of Middle C: 256 Hertz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287911f9",
   "metadata": {},
   "source": [
    "# What about an actual song?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(r\"data/levitating.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee8e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs, song = read(\"data/levitating.wav\")\n",
    "song = song[:, 0]\n",
    "\n",
    "time_to_plot = np.arange(0, 150000, dtype=int)\n",
    "idx, song_ft = transform_song(Fs, song[time_to_plot])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 5))\n",
    "ax[0].plot(time_to_plot, song[time_to_plot])\n",
    "ax[1].plot(idx, song_ft)\n",
    "ax[1].set_xlim([0, 5000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1687f406",
   "metadata": {},
   "source": [
    "# How can we use this information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d09ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find peaks in the frequencies of fft-transformed song\n",
    "def find_peaks(amplitudes):\n",
    "\n",
    "    peak_indices, props = signal.find_peaks(amplitudes, prominence=0, distance=10000)\n",
    "\n",
    "    # take only the 15 most prominent peaks\n",
    "    n_peaks = 15\n",
    "\n",
    "    # peak choosing method taken from links below:\n",
    "    # https://michaelstrauss.dev/shazam-in-python\n",
    "    # https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "    candidate_peaks = props[\"prominences\"]\n",
    "    if len(candidate_peaks) < n_peaks:\n",
    "        n_peaks = len(candidate_peaks)\n",
    "    largest_peak_indices = np.argpartition(props[\"prominences\"], -n_peaks)[-n_peaks:]\n",
    "    return peak_indices[largest_peak_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459cd894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_ft(fx, fy, peaks=()):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.plot(fx, fy, zorder=1)\n",
    "    plt.scatter(fx[peaks], fy[peaks], color=\"red\", zorder=2)\n",
    "    ax.set_xlim([0, 5000])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca91a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, song_ft = transform_song(Fs, song)\n",
    "peaks = find_peaks(song_ft)\n",
    "graph_ft(idx, song_ft, peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6039040",
   "metadata": {},
   "source": [
    "# Creating a unique \"fingerprint\" for a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constellation(audio, Fs):\n",
    "    # Parameters\n",
    "    window_length_seconds = 0.5\n",
    "    window_length_samples = int(window_length_seconds * Fs)\n",
    "    window_length_samples += window_length_samples % 2\n",
    "    num_peaks = 15\n",
    "\n",
    "    # Pad the song to divide evenly into windows\n",
    "    amount_to_pad = window_length_samples - audio.size % window_length_samples\n",
    "\n",
    "    song_input = np.pad(audio, (0, amount_to_pad))\n",
    "\n",
    "    # Perform a short time fourier transform\n",
    "    frequencies, times, stft = signal.stft(\n",
    "        song_input, Fs, nperseg=window_length_samples, nfft=window_length_samples, return_onesided=True\n",
    "    )\n",
    "\n",
    "    constellation_map = []\n",
    "\n",
    "    for time_idx, window in enumerate(stft.T):\n",
    "        # Spectrum is by default complex. \n",
    "        # We want real values only\n",
    "        spectrum = abs(window)\n",
    "        # Find peaks - these correspond to interesting features\n",
    "        # Note the distance - want an even spread across the spectrum\n",
    "        peaks, props = signal.find_peaks(spectrum, prominence=0, distance=200)\n",
    "\n",
    "        # Only want the most prominent peaks\n",
    "        # With a maximum of 15 per time slice\n",
    "        n_peaks = min(num_peaks, len(peaks))\n",
    "        # Get the n_peaks largest peaks from the prominences\n",
    "        # This is an argpartition\n",
    "        # Useful explanation: https://kanoki.org/2020/01/14/find-k-smallest-and-largest-values-and-its-indices-in-a-numpy-array/\n",
    "        largest_peaks = np.argpartition(props[\"prominences\"], -n_peaks)[-n_peaks:]\n",
    "        for peak in peaks[largest_peaks]:\n",
    "            frequency = frequencies[peak]\n",
    "            constellation_map.append([time_idx, frequency])\n",
    "\n",
    "    return constellation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs, song = read(\"data/levitating.wav\")\n",
    "# audio can have two channels, use only one\n",
    "if song.shape[1] > 1:\n",
    "    song = song[:, 0]\n",
    "\n",
    "constellation_map = create_constellation(song, Fs)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylabel(\"Frequency (Hz)\")\n",
    "ax.set_xlabel(\"Time Step\")\n",
    "plt.scatter(*zip(*constellation_map))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6693d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47869610-0d68-4ae4-b084-62da5acec8e6",
   "metadata": {},
   "source": [
    "From Aydin, we have the means to construct the constellation for hashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc108d4e-eea7-4b81-b99c-3fe422c38906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constellation(audio, Fs):\n",
    "    window_length_seconds = 0.5\n",
    "    window_length_samples = int(window_length_seconds * Fs)\n",
    "    window_length_samples += window_length_samples % 2\n",
    "    num_peaks = 15\n",
    "\n",
    "    amount_to_pad = window_length_samples - audio.size % window_length_samples\n",
    "\n",
    "    song_input = np.pad(audio, (0, amount_to_pad))\n",
    "\n",
    "    frequencies, times, stft = signal.stft(\n",
    "        song_input, Fs, nperseg=window_length_samples, nfft=window_length_samples, return_onesided=True\n",
    "    )\n",
    "\n",
    "    constellation_map = []\n",
    "\n",
    "    for time_idx, window in enumerate(stft.T):\n",
    "        spectrum = abs(window)\n",
    "        peaks, props = signal.find_peaks(spectrum, prominence=0, distance=200)\n",
    "\n",
    "        n_peaks = min(num_peaks, len(peaks))\n",
    "        largest_peaks = np.argpartition(props[\"prominences\"], -n_peaks)[-n_peaks:]\n",
    "        for peak in peaks[largest_peaks]:\n",
    "            frequency = frequencies[peak]\n",
    "            constellation_map.append([time_idx, frequency])\n",
    "\n",
    "    return constellation_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b1ae1-7654-4002-932c-843563f3e1fd",
   "metadata": {},
   "source": [
    "With a constellation, we can decide an inital way on how we want to construct our hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789b7ea-3bd2-4717-8d19-6bbc8fdef198",
   "metadata": {},
   "outputs": [],
   "source": [
    "constellation_map = create_constellation(song, Fs)\n",
    "\n",
    "def create_hashes(constellation_map, song_id=None):\n",
    "    hashes = {}\n",
    "    upper_frequency = 23_000 \n",
    "    frequency_bits = 10\n",
    "\n",
    "    \n",
    "    for idx, (time, freq) in enumerate(constellation_map):\n",
    "        for other_time, other_freq in constellation_map[idx : idx + 100]: \n",
    "            diff = other_time - time\n",
    "            if diff <= 1 or diff > 10:\n",
    "                continue\n",
    "\n",
    "            freq_binned = freq / upper_frequency * (2 ** frequency_bits)\n",
    "            other_freq_binned = other_freq / upper_frequency * (2 ** frequency_bits)\n",
    "\n",
    "            hash = int(freq_binned) | (int(other_freq_binned) << 10) | (int(diff) << 20)\n",
    "            hashes[hash] = (time, song_id)\n",
    "    return hashes\n",
    "\n",
    "hashes = create_hashes(constellation_map, 0)\n",
    "for i, (hash, (time, _)) in enumerate(hashes.items()):\n",
    "    if i > 10: \n",
    "        break\n",
    "    print(f\"Hash {hash} occurred at {time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0092c-fee3-4d76-806e-e4b00d02c6ad",
   "metadata": {},
   "source": [
    "Next, we take a folder of .wav files and hash them, storing the data into a hashtable database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78814846-ed26-47d1-a16d-67d0aad8f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "songs = glob.glob('wavdata/*.wav')\n",
    "\n",
    "song_name_index = {}\n",
    "database: Dict[int, List[Tuple[int, int]]] = {}\n",
    "\n",
    "for index, filename in enumerate(tqdm(sorted(songs))):\n",
    "    song_name_index[index] = filename\n",
    "    Fs, audio_input = read(filename)\n",
    "    songdf=pd.DataFrame(audio_input)\n",
    "    audio_input=songdf.iloc[:,1]+songdf.iloc[:,0]\n",
    "    constellation = create_constellation(audio_input, Fs)\n",
    "    hashes = create_hashes(constellation, index)\n",
    "\n",
    "    for hash, time_index_pair in hashes.items():\n",
    "        if hash not in database:\n",
    "            database[hash] = []\n",
    "        database[hash].append(time_index_pair)\n",
    "        \n",
    "with open(\"database.pickle\", 'wb') as db:\n",
    "    pickle.dump(database, db, pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"song_index.pickle\", 'wb') as songs:\n",
    "    pickle.dump(song_name_index, songs, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae6edc-a8d9-4e3c-a8c5-e9b9f68f1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pickle.load(open('database.pickle', 'rb'))\n",
    "song_name_index = pickle.load(open(\"song_index.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60323c-f64e-4fe7-ad72-cc2af7043606",
   "metadata": {},
   "source": [
    "Lastly, when we wish to use a recording, we hash the recording with the same parameters and then attempt to find matching hashes from our song database in order to find the one that is closest to our recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e762059-008d-43e9-802c-528b6c1e83eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs, audio_input = read(\"data/recording2.wav\")\n",
    "constellation = create_constellation(audio_input, Fs)\n",
    "hashes = create_hashes(constellation, None)\n",
    "\n",
    "matches_per_song = {}\n",
    "for hash, (sample_time, _) in hashes.items():\n",
    "    if hash in database:\n",
    "        matching_occurences = database[hash]\n",
    "        for source_time, song_id in matching_occurences:\n",
    "            if song_id not in matches_per_song:\n",
    "                matches_per_song[song_id] = 0\n",
    "            matches_per_song[song_id] += 1\n",
    "\n",
    "for song_id, num_matches in list(sorted(matches_per_song.items(), key=lambda x: x[1], reverse=True))[:10]:\n",
    "    print(f\"Song: {song_name_index[song_id]} - Matches: {num_matches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7388d-25f8-4e35-88bf-b21edb088452",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('data/recording2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986b731-e50b-46fe-8c3b-7f01dd90a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('wavdata/baby.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808ae20-6992-40c9-97b0-c100a24602f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "639882a7-cd81-41c2-b156-e7497dc5d599",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Issue of Finding Matches For Recordings with Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6942c90a-8c2a-46f1-9226-eced81be35ba",
   "metadata": {},
   "source": [
    "### When adding more nosies.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6aa4a0-e65b-4d6c-9bb6-7670741f7f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('data/noise_talking.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c524758-0890-4f16-a53e-cabc8abb6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio('data/baby_trim1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1afa56-0cbb-4281-bc83-c784b0904f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read\n",
    "import pandas as pd\n",
    "Fs_noise, audio_input_noise = read(\"data/noise_talking.wav\")\n",
    "Fs, audio_input = read(\"data/baby_trim1.wav\")\n",
    "songdf=pd.DataFrame(audio_input)\n",
    "audio_input=songdf.iloc[:,1]+songdf.iloc[:,1]\n",
    "noisemax=max(audio_input_noise)\n",
    "audiomax=max(audio_input)\n",
    "# random_noise=np.random.randint(-1000, high=1000, size=len(audio_input_noise), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41195b66-0764-4d80-ba35-4861e35595c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio=audio_input/audiomax+audio_input_noise[-len(audio_input):]/noisemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6853d9d5-5af1-4f14-95ea-d82507e0490c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4adfbcb-fbf4-474d-8eb1-d6e590d035dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_input=audio\n",
    "constellation = create_constellation(audio_input, Fs)\n",
    "hashes = create_hashes(constellation, None)\n",
    "\n",
    "# For each hash in the song, check if there's a match in the database\n",
    "# There could be multiple matching tracks, so for each match:\n",
    "#   Incrememnt a counter for that song ID by one\n",
    "matches_per_song = {}\n",
    "for hash, (sample_time, _) in hashes.items():\n",
    "    if hash in database:\n",
    "        matching_occurences = database[hash]\n",
    "        for source_time, song_id in matching_occurences:\n",
    "            if song_id not in matches_per_song:\n",
    "                matches_per_song[song_id] = 0\n",
    "            matches_per_song[song_id] += 1\n",
    "\n",
    "for song_id, num_matches in list(sorted(matches_per_song.items(), key=lambda x: x[1], reverse=True)):\n",
    "    print(f\"Song: {song_name_index[song_id]} - Matches: {num_matches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89508f87-6688-44c7-921c-273aa5f8d07b",
   "metadata": {},
   "source": [
    "The matches number reduced a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa88c04-07c7-420a-8018-a68bd3e7a666",
   "metadata": {},
   "source": [
    "# Another way: Time Differenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df818834-5d8b-4dc0-bb9b-b28046649a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a short recording with some background noise\n",
    "# this method make use of the characterisitic of songs, they have a \n",
    "# specific order to occur\n",
    "audio_input=audio\n",
    "constellation = create_constellation(audio_input, Fs)\n",
    "hashes = create_hashes(constellation, None)\n",
    "\n",
    "# For each hash in the song, check if there's a match in the database\n",
    "# There could be multiple matches, so for each match:\n",
    "#   Append all of them to a hashmap based on the song id along with the time\n",
    "#   the hash occurs in the sample and at the source\n",
    "# In the end, matches_per_song is key'd by song ID with values being\n",
    "# lists of hashes, the \n",
    "matches_per_song = {}\n",
    "for hash, (sample_time, _) in hashes.items():\n",
    "    if hash in database:\n",
    "        matching_occurences = database[hash]\n",
    "        for source_time, song_id in matching_occurences:\n",
    "            if song_id not in matches_per_song:\n",
    "                matches_per_song[song_id] = []\n",
    "            matches_per_song[song_id].append((hash, sample_time, source_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5071f742-d8e4-458b-8b74-d10072b9c895",
   "metadata": {},
   "source": [
    "We try to find time index in database having the hash code as the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ea11e-84f4-4b3e-a49d-5cede0e24815",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "song_ids = [2,3,4, 10] # Song ID=10 is the true match\n",
    "for song_id in song_ids:\n",
    "    song_name = song_name_index[song_id].split('/')[1]\n",
    "\n",
    "    matches = matches_per_song[song_id]\n",
    "    print(f\"Total matches for {song_name}: {len(matches)}\")\n",
    "    song_scores_by_offset = {}\n",
    "    for hash, sample_time, source_time in matches:\n",
    "        delta = source_time - sample_time\n",
    "        if delta not in song_scores_by_offset:\n",
    "            song_scores_by_offset[delta] = 0\n",
    "        song_scores_by_offset[delta] += 1\n",
    "\n",
    "    # Produce a histogram\n",
    "    # For clarity's sake, only plot the 100 largest offsets\n",
    "    high_scores = list(sorted(song_scores_by_offset.items(), key=lambda x: x[1], reverse=True))[:100]\n",
    "    plt.figure()\n",
    "    plt.bar(*zip(*high_scores))\n",
    "    plt.title(song_name)\n",
    "    plt.ylim((0, 900))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f4141-a491-4936-bd23-a4368bf70014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
